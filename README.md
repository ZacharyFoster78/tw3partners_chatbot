
# TW3Partners Chatbot

Un chatbot intelligent capable d‚Äôinteragir avec l‚Äôutilisateur, d‚Äôeffectuer des recherches sur Internet et de g√©n√©rer des r√©ponses enrichies par des r√©sultats web. Il utilise un mod√®le LLM local (Qwen2.5 via Ollama) pour comprendre les requ√™tes, interagir avec SerpAPI pour r√©cup√©rer des r√©sultats web, et int√®gre une interface React pour l‚Äôexp√©rience utilisateur.

---

## üìö Sommaire

- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Installation](#installation)
  - [Pr√©requis](#pr√©requis)
  - [Frontend](#frontend)
  - [Backend](#backend)
- [Aper√ßu](#aper√ßu)
- [Pipeline de traitement](#pipeline-de-traitement)
- [D√©ploiement Azure](#d√©ploiement-azure)
  - [√âtapes de d√©ploiement](#√©tapes-de-d√©ploiement)
  - [Services Azure utilis√©s](#services-azure-utilis√©s)
  - [Estimation des co√ªts](#estimation-des-co√ªts)
  - [Acc√®s requis](#acc√®s-requis)
- [Mise en production](#mise-en-production)
  - [CI/CD Pipeline](#cicd-pipeline)
  - [Monitoring & Logs](#monitoring--logs)
  - [Gestion des erreurs](#gestion-des-erreurs)
  - [Sauvegarde & Backup](#sauvegarde--backup)

---

## üß† Fonctionnalit√©s

- Requ√™tes utilisateur interpr√©t√©es par un LLM local (Qwen2.5).
- G√©n√©ration de mots-cl√©s pour requ√™tes web.
- Utilisation de SerpAPI pour interroger un moteur de recherche.
- G√©n√©ration de r√©ponse en langage naturel bas√©e sur les r√©sultats.
- Interface utilisateur React.
- Feedback utilisateur localement sauvegard√©.

---

## ‚öôÔ∏è Installation

### Pr√©requis

- Node.js
- Python 3.13
- [Ollama](https://ollama.com/) (avec le mod√®le Qwen2.5)
- Cl√© API [SerpAPI](https://serpapi.com/)

### Frontend

```bash
cd frontend
npm install
npm start
```

### Backend

```bash
cd backend
pip install -r requirements.txt
```

Cr√©er un fichier `.env` dans le dossier `backend` avec la cl√© SerpAPI :

```
SERPAPI_KEY=your_api_key_here
```

Lancer le backend :

```bash
uvicorn app.main:app --reload
```

---

## üñºÔ∏è R√©sultats

Voici un aper√ßu des r√©sultats :

![Capture d'√©cran du chatbot](image.png)

---

## üîÑ Pipeline de traitement

1. **Requ√™te utilisateur** envoy√©e via l‚Äôinterface.
2. **Extraction des mots-cl√©s** par le LLM.
3. **Recherche Web** via SerpAPI avec ces mots-cl√©s.
4. **R√©cup√©ration des 10 meilleurs r√©sultats**.
5. **G√©n√©ration de r√©ponse** par le LLM bas√©e sur les r√©sultats.
6. **Affichage de la r√©ponse** √† l‚Äôutilisateur.
7. **Feedback utilisateur** optionnel et sauvegard√© localement.

---

## ‚òÅÔ∏è D√©ploiement Azure

### üìù Remarques

> - Le mod√®le Qwen2.5 est ex√©cut√© localement via Ollama. Pour des raisons de co√ªt et de simplicit√© de d√©ploiement, j'ai r√©pondu aux questions comme si j'avais utilis√© une API OpenRouter. Il est possible de deployer Ollama avec une image dockeriser.
> - Bien que l'IA ne soit pas autoris√© pour r√©pondre aux questions de la seconde partie, je l'ai utilis√© pour le visuel.

### √âtapes de d√©ploiement

1. Modifier le code : changer les appels API locaux de `/localhost` vers `/api`.
2. Dockeriser le backend pour le d√©ployer sur Azure Container Apps.
3. Publier le code sur GitHub.
4. Cr√©er un **groupe de ressources** sur Azure.
5. Mettre en place **Azure Key Vault** pour stocker les cl√©s API.
6. Cr√©er un **Azure Container Registry** pour h√©berger les images Docker.
7. Cr√©er un environnement **Azure Container App** o√π d√©ployer le backend
8. D√©ployer le frontend via **Azure Static Web App** connect√© √† GitHub et le lier au backend.
9. Optionnel : configurer **Azure Storage** pour enregistrer les feedbacks.

### Services Azure utilis√©s

| Service                  | Raison                                                                 |
|--------------------------|----------------------------------------------------------------------|
| Azure Container Registry | Stockage des images Docker pratique pour CI/CD, les containers permettent une architecture scalable|
| Azure Container Apps     | Ex√©cution du backend en conteneur                                    |
| Azure Static Web Apps    | H√©bergement React, support CI/CD automatique, gratuit                |
| Azure Key Vault          | Stockage s√©curis√© des cl√©s API                                       |
| Azure Storage (optionnel)| Stockage des feedbacks ou conversations utilisateur                  |

### Estimation des co√ªts

Sans savoir la taille du projet, il est difficile d'estimer les couts.

| √âl√©ment                  | Co√ªt estim√©                                |
|--------------------------|---------------------------------------------|
| Azure Container Apps     | Gratuit jusqu‚Äô√† ~2M de requ√™tes/mois        |
| Azure Static Web App     | Gratuit pour projets simples                |
| Azure Key Vault          | Faible (0.03/10000 transaction)             |
| SerpAPI                  | D√©pend de l‚Äôusage                           |
| OpenRouter (si utilis√©)  | Payant selon usage si LLM h√©berg√© √† distance |

### Acc√®s requis

- Acc√®s au portail Azure
- Acc√®s au repository GitHub
- Azure Key Vault
- Azure Container Registry
- Azure Container App
- Azure Static Web App
- Azure Storage (optionnel)
- Cl√© API SerpAPI
- OpenRouter API Key (si applicable)

---

## üöÄ Mise en production

### CI/CD Pipeline

Mise en place via **GitHub Actions** :

- Tests automatiques (ex. ping des endpoints).
- Build Docker de l‚ÄôAPI.
- Push de l‚Äôimage vers Azure Container Registry.
- D√©ploiement automatique via Azure.

### Monitoring & Logs

- Utiliser **Azure Monitor** et **Log Analytics**.
- Suivi des performances (latence, erreurs 500, temps de r√©ponse...).

### Gestion des erreurs

- Impl√©menter un syst√®me d‚Äôalerte (mail ou webhook).
- Centraliser les logs backend.
- Reproduire les erreurs via journaux d√©taill√©s.

### Sauvegarde & Backup

- Le versioning GitHub assure la tra√ßabilit√© du code.
- Sauvegardes locales possibles pour les feedbacks utilisateurs.
- Azure Storage pour stocker et versionner les feedbacks.

---


## üß© Axes d'am√©lioration

Le projet peut √©voluer sur plusieurs aspects techniques pour gagner en performance, en pr√©cision et en ergonomie :

1. **Pr√©chargement du LLM**  
   Actuellement, le mod√®le est charg√© √† la vol√©e, ce qui ralentit les premi√®res r√©ponses. Pr√©charger le LLM au d√©marrage du backend permettrait de r√©duire drastiquement le temps de r√©ponse initial.

2. **Extraction approfondie du contenu web**  
   Le syst√®me se limite aujourd‚Äôhui aux descriptions fournies par les moteurs de recherche. En acc√©dant directement aux pages web et en extrayant les **3 premiers paragraphes**, le LLM pourrait g√©n√©rer des r√©ponses bien plus pr√©cises et contextuelles.

3. **Mise en place d‚Äôun syst√®me de cache**  
   Ajouter un m√©canisme de **caching intelligent** (bas√© sur les questions ou les mots-cl√©s) permettrait :
   - d'acc√©l√©rer les r√©ponses aux requ√™tes r√©currentes,
   - d'am√©liorer la continuit√© conversationnelle,
   - de r√©duire les appels √† SerpAPI.

4. **Sauvegarde des conversations en base de donn√©es**  
   Aujourd‚Äôhui, les feedbacks peuvent √™tre sauvegard√©s localement, mais aucune base ne conserve l‚Äôhistorique complet. Ajouter une base de donn√©es permettrait :
   - de reprendre des conversations,
   - d‚Äôexploiter les logs pour du fine-tuning ou de l‚Äôanalyse,
   - de mieux personnaliser l‚Äôexp√©rience utilisateur.
